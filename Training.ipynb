{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kaggle.json') as f:\n",
    "    key = json.load(f)[\"Roboflow\"]\n",
    "rf = Roboflow(api_key=key)\n",
    "\n",
    "project = rf.workspace(\"dmay\").project(\"cctv_car_bike_detection-jzjis\")\n",
    "dataset = project.version(3).download(\"yolov8\", \"./datasets/car_bike\")\n",
    "\n",
    "project = rf.workspace(\"ujjawal\").project(\"seat-belt-2-77oye\")\n",
    "dataset = project.version(5).download(\"yolov8\", \"./datasets/seatbelt\")\n",
    "\n",
    "project = rf.workspace(\"kalay-highschool\").project(\"yahli-helmet-no-helmet\")\n",
    "dataset = project.version(1).download(\"yolov8\", \"./datasets/helmet\")\n",
    "\n",
    "project = rf.workspace(\"muhammad-rizki\").project(\"plat-nomor-detection\")\n",
    "dataset = project.version(2).download(\"yolov8\", \"./datasets/plat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/mnt/c/Users/sahri/OneDrive/Documents/VSCode/Bangkit/Machine_learning/datasets/\"\n",
    "model = YOLO('./datasets/yolov8s.pt')\n",
    "results = model.train(data=PATH+'car_bike/data.yaml', epochs=100, batch=32, imgsz=960, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/mnt/c/Users/sahri/OneDrive/Documents/VSCode/Bangkit/Machine_learning/datasets/\"\n",
    "model = YOLO('./datasets/yolov8s.pt')\n",
    "results = model.train(data=PATH+'helmet/32.yaml', epochs=100, batch=32, imgsz=960, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/mnt/c/Users/sahri/OneDrive/Documents/VSCode/Bangkit/Machine_learning/datasets/\"\n",
    "model = YOLO('./datasets/yolov8s.pt')\n",
    "results = model.train(data=PATH+'plat/32.yaml', epochs=100, batch=32, imgsz=960, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/mnt/c/Users/sahri/OneDrive/Documents/VSCode/Bangkit/Machine_learning/datasets/\"\n",
    "model = YOLO('./datasets/yolov8s.pt')\n",
    "results = model.train(data=PATH+'seatbelt/data.yaml', epochs=100, batch=32, imgsz=960, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import cv2\n",
    "from keras.optimizers import SGD\n",
    "import opendatasets\n",
    "\n",
    "project = 'https://www.kaggle.com/sachinpatel21/az-handwritten-alphabets-in-csv-format'\n",
    "opendatasets.download(project, \"./datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_az_dataset(datasetPath):\n",
    "\tdata = []\n",
    "\tlabels = []\n",
    "\n",
    "\tfor row in open(datasetPath):\n",
    "\t\trow = row.split(\",\")\n",
    "\t\tlabel = int(row[0])\n",
    "\t\timage = np.array([int(x) for x in row[1:]], dtype=\"uint8\")\n",
    "\t\timage = image.reshape((28, 28))\n",
    "\t\tdata.append(image)\n",
    "\t\tlabels.append(label)\n",
    "\n",
    "\tdata = np.array(data, dtype=\"float32\")\n",
    "\tlabels = np.array(labels, dtype=\"int\")\n",
    "\treturn (data, labels)\n",
    "\n",
    "def load_mnist_dataset():\n",
    "\t((trainData, trainLabels), (testData, testLabels)) = tf.keras.datasets.mnist.load_data()\n",
    "\tdata = np.vstack([trainData, testData])\n",
    "\tlabels = np.hstack([trainLabels, testLabels])\n",
    "\treturn (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(azData, azLabels) = load_az_dataset(\"./datasets/az-handwritten-alphabets-in-csv-format/A_Z Handwritten Data.csv\")\n",
    "(digitsData, digitsLabels) = load_mnist_dataset()\n",
    "\n",
    "azLabels += 10\n",
    "data = np.vstack([azData, digitsData])\n",
    "labels = np.hstack([azLabels, digitsLabels])\n",
    "data = [cv2.resize(image, (32, 32)) for image in data]\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "data = np.expand_dims(data, axis=-1)\n",
    "data /= 255.0\n",
    "\n",
    "le = LabelBinarizer()\n",
    "labels = le.fit_transform(labels)\n",
    "counts = labels.sum(axis=0)\n",
    "classTotals = labels.sum(axis=0)\n",
    "classWeight = {}\n",
    "for i in range(0, len(classTotals)):\n",
    "\tclassWeight[i] = classTotals.max() / classTotals[i]\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.20, stratify=labels, random_state=42)\n",
    "\n",
    "aug = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "\trotation_range=10,\n",
    "\tzoom_range=0.05,\n",
    "\twidth_shift_range=0.1,\n",
    "\theight_shift_range=0.1,\n",
    "\tshear_range=0.15,\n",
    "\thorizontal_flip=False,\n",
    "\tfill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_module(data, K, stride, red=False, reg=0.0001):\n",
    "    shortcut = data\n",
    "    bn1 = tf.keras.layers.BatchNormalization(axis=-1)(data)\n",
    "    act1 = tf.keras.layers.Activation(\"relu\")(bn1)\n",
    "    conv1 = tf.keras.layers.Conv2D(int(K * 0.25), (1, 1), use_bias=False, kernel_regularizer=tf.keras.regularizers.l2(reg))(act1)\n",
    "\n",
    "    bn2 = tf.keras.layers.BatchNormalization(axis=-1)(conv1)\n",
    "    act2 = tf.keras.layers.Activation(\"relu\")(bn2)\n",
    "    conv2 = tf.keras.layers.Conv2D(int(K * 0.25), (3, 3), strides=stride, padding=\"same\", use_bias=False, kernel_regularizer=tf.keras.regularizers.l2(reg))(act2)\n",
    "\n",
    "    bn3 = tf.keras.layers.BatchNormalization(axis=-1)(conv2)\n",
    "    act3 = tf.keras.layers.Activation(\"relu\")(bn3)\n",
    "    conv3 = tf.keras.layers.Conv2D(K, (1, 1), use_bias=False, kernel_regularizer=tf.keras.regularizers.l2(reg))(act3)\n",
    "\n",
    "    if red:\n",
    "        shortcut = tf.keras.layers.Conv2D(K, (1, 1), strides=stride, use_bias=False, kernel_regularizer=tf.keras.regularizers.l2(reg))(act1)\n",
    "    x = tf.keras.layers.add([conv3, shortcut])\n",
    "    return x\n",
    "\n",
    "stages = [3, 3, 3]\n",
    "filters = [64, 64, 128, 256]\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(32, 32, 1))\n",
    "x = tf.keras.layers.BatchNormalization(axis=-1)(inputs)\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3), use_bias=False, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.0001))(x)\n",
    "\n",
    "for i in range(0, len(stages)):\n",
    "    stride = (1, 1) if i == 0 else (2, 2)\n",
    "    x = residual_module(x, filters[i + 1], stride, red=True)\n",
    "\n",
    "    for _ in range(0, stages[i] - 1):\n",
    "        x = residual_module(x, filters[i + 1], (1, 1))\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "x = tf.keras.layers.AveragePooling2D((8, 8))(x)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(len(le.classes_), kernel_regularizer=tf.keras.regularizers.l2(0.0001))(x)\n",
    "x = tf.keras.layers.Activation(\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=SGD(learning_rate=0.1, weight_decay=0.002),\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "H = model.fit(\n",
    "\taug.flow(trainX, trainY, batch_size=128),\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tsteps_per_epoch=len(trainX) // 128,\n",
    "\tepochs=10,\n",
    "\tclass_weight=classWeight,\n",
    "\tverbose=1)\n",
    "\n",
    "model.save(\"./datasets/ocr.h5\", save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.222 ðŸš€ Python-3.9.18 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24576MiB)\n",
      "Model summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/sahri/OneDrive/Documents/VSCode/Bangkit/Machine_learning/datasets/car_bike/valid/labels.cache... 159 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 159/159 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 30, len(boxes) = 1854. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:22<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        159       1854      0.848       0.84      0.889       0.58\n",
      "                  bike        159        373      0.851      0.831      0.878      0.549\n",
      "                   car        159       1481      0.845      0.849        0.9      0.612\n",
      "Speed: 2.4ms preprocess, 118.3ms inference, 0.0ms loss, 2.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val\u001b[0m\n",
      "Ultralytics YOLOv8.0.222 ðŸš€ Python-3.9.18 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24576MiB)\n",
      "Model summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/sahri/OneDrive/Documents/VSCode/Bangkit/Machine_learning/datasets/helmet/valid/labels.cache... 49 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:10<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         49        145      0.772      0.589      0.672      0.384\n",
      "           With Helmet         49         37      0.798      0.427      0.578      0.333\n",
      "        Without Helmet         49        108      0.747       0.75      0.765      0.434\n",
      "Speed: 3.3ms preprocess, 183.1ms inference, 0.0ms loss, 2.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n",
      "Ultralytics YOLOv8.0.222 ðŸš€ Python-3.9.18 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24576MiB)\n",
      "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/sahri/OneDrive/Documents/VSCode/Bangkit/Machine_learning/datasets/plat/valid/labels.cache... 181 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181/181 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:30<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        181        224      0.966      0.946      0.974        0.8\n",
      "Speed: 1.2ms preprocess, 154.1ms inference, 0.0ms loss, 2.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val3\u001b[0m\n",
      "Ultralytics YOLOv8.0.222 ðŸš€ Python-3.9.18 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24576MiB)\n",
      "Model summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Users/sahri/OneDrive/Documents/VSCode/Bangkit/Machine_learning/datasets/seatbelt/valid/labels.cache... 148 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        151      0.997          1      0.995      0.722\n",
      "              seatbelt        148         74      0.998          1      0.995      0.696\n",
      "           no-seatbelt        148         77      0.996          1      0.995      0.748\n",
      "Speed: 3.1ms preprocess, 143.3ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val4\u001b[0m\n",
      "692/692 [==============================] - 9s 12ms/step - loss: 0.3299 - accuracy: 0.9233\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "car_bike = YOLO(\"./runs/detect/train/weights/best.pt\")\n",
    "helmet = YOLO(\"./runs/detect/train2/weights/best.pt\")\n",
    "plat = YOLO(\"./runs/detect/train3/weights/best.pt\")\n",
    "seatbelt = YOLO(\"./runs/detect/train4/weights/best.pt\")\n",
    "ocr = tf.keras.models.load_model(\"./datasets/ocr.h5\")\n",
    "\n",
    "metrics1 = car_bike.val()\n",
    "metrics2 = helmet.val()\n",
    "metrics3 = plat.val()\n",
    "metrics4 = seatbelt.val()\n",
    "metrics5 = ocr.evaluate(testX, testY, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp \"./runs/detect/train/weights/best.pt\" \"./models/vehicle.pt\"\n",
    "!cp \"./runs/detect/train2/weights/best.pt\" \"./models/helmet.pt\"\n",
    "!cp \"./runs/detect/train3/weights/best.pt\" \"./models/plate.pt\"\n",
    "!cp \"./runs/detect/train4/weights/best.pt\" \"./models/seatbelt.pt\"\n",
    "!cp \"./runs/detect/train/confusion_matrix_normalized.png\" \"./models/confusion_matrix_normalized_vehicle.png\"\n",
    "!cp \"./runs/detect/train2/confusion_matrix_normalized.png\" \"./models/confusion_matrix_normalized_helmet.png\"\n",
    "!cp \"./runs/detect/train3/confusion_matrix_normalized.png\" \"./models/confusion_matrix_normalized_plate.png\"\n",
    "!cp \"./runs/detect/train4/confusion_matrix_normalized.png\" \"./models/confusion_matrix_normalized_seatbelt.png\"\n",
    "!cp \"./datasets/ocr.h5\" \"./models/ocr.h5\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sahriar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
